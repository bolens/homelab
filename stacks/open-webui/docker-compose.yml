# Open WebUI - Extensible, feature-rich, user-friendly self-hosted AI platform
# Supports Ollama and OpenAI-compatible APIs
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    # No host port by default: use Caddy (open-webui:8080). To expose on host, set OPEN_WEBUI_HOST_PORT in .env and uncomment:
    # ports:
    #   - "${OPEN_WEBUI_HOST_PORT}:8080"
    volumes:
      - open_webui_data:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - default
      - monitor
    environment:
      - TZ=${TZ:-America/Denver}
      - LANG=${LANG:-en_US.UTF-8}
      - LC_ALL=${LC_ALL:-en_US.UTF-8}
      - LC_CTYPE=${LC_CTYPE:-en_US.UTF-8}
      # Ollama Configuration
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      # OpenAI Configuration (optional)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      # Hugging Face (optional; enables higher rate limits and faster HF Hub downloads)
      - HF_TOKEN=${HF_TOKEN:-}
      # Database Configuration (optional, defaults to SQLite)
      - DATA_DIR=/app/backend/data
      # Authentication (optional)
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-user}
      # Additional configuration
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-}
      - WEBUI_JWT_SECRET_KEY=${WEBUI_JWT_SECRET_KEY:-}

networks:
  monitor:
    name: monitor
    external: true

volumes:
  open_webui_data:
