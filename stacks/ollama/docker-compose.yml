# Ollama - Local LLM runtime (CPU by default; optional GPU)
# Models: configurable via OLLAMA_MODELS_PATH. Other data: Docker volume ollama_data.
# To use NVIDIA GPU: install NVIDIA Container Toolkit, then uncomment the "deploy" block below.
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_HOST_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ${OLLAMA_MODELS_PATH:-./models}:/root/.ollama/models:rw
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - default
      - monitor
    environment:
      - TZ=${TZ:-America/Denver}
      - LANG=${LANG:-en_US.UTF-8}
      - LC_ALL=${LC_ALL:-en_US.UTF-8}
      - LC_CTYPE=${LC_CTYPE:-en_US.UTF-8}
      - OLLAMA_HOST=0.0.0.0:11434
    # NVIDIA GPU: uncomment the block below when NVIDIA Container Toolkit is installed
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  monitor:
    name: monitor
    external: true

volumes:
  ollama_data: {}
